# CVPR2022-Degradation-agnostic Correspondence from Resolution-asymmetric Stereo

# 0. 摘要

+ 研究问题：一对不同分辨率图像的立体匹配；

+ 方法：无监督学习、特征度量一致性、自增强优化策略；

+ 验证：在各种退化的模拟数据集和自收集的真实数据集上的实验验证了所提方法优于现有解决方案。

> [论文地址](https://arxiv.org/abs/2204.01429)

# 1. 介绍

+ 研究背景及意义：目前，由两个（或多个）不同焦距镜头组成的远距离相机系统广泛应用于智能手机中。这类系统通常在一次拍摄中生成一对（或一组）具有不同分辨率的图像，这使得许多理想的应用成为可能，例如**连续光学变焦**和**图像质量增强**。对于这些应用，分辨率不对称立体图像的对应点估计是一个关键步骤，通常由**传统对称立体匹配算法（如SGM)**和**图像上采样**来进行。然而这种方法很容易受到图像上采样引入的伪影影响，当上采样范围很大时这种影响更加明显。

+ 研究方法介绍：

  + 无监督学习：非对称立体匹配在一些特定的背景下进行了研究，如**辐射变差**和**模态差**。在本文中，我们重点讨论了分辨率非对称的情况，它很实用但很少被明确地研究。近期Liu等人，提出一个统一的网络进行有监督学习，用于视觉不平衡立体匹配，以解决单目模糊和噪声。但它不仅需要视差真值和高质量的退化视图作为标签，而且还需要显式退化形式来学习网络参数，这使得它很难适用于很少获得可视信息的各种现实世界系统。因此，我们转向无监督学习的方向。

  + 特征度量一致性：对于无监督立体匹配，最广泛采用的假设是光度一致性。如下图1(a)所示，在对称立体对中，$I_{L}[p_{L}]=I_{R}[p_{R}]$，然而对于分辨率不对称的立体对，这一假设可能不成立，其中低分辨率(LR)视图与高分辨率(HR)视图相比，由于未知的下采样核而降低了低分辨率(LR)视图。换句话说，非对称立体对中的对应像素（例如，图1(b)中的$p_{L}$和$p_{R}$ ）可以不具有相同的强度或颜色，这种光度不一致会给对应点学习带来新的困难。现有的解决方法是通过超分辨率(SR)技术将LR视图恢复到HR视图。然而，现有的SR方法大多是退化特定的，如果实际退化与假设的退化不同（对于非盲SR）或不在假设的范围内（对于盲SR），性能就会下降。因此，在实际应用中，SR方法弥补光度不一致的有效性将受到阻碍。为了克服上述挑战，**我们提出了通过在特征空间中而不是在图像空间中施加两个视图的一致性**，从一个新的角度来解决分辨率非对称立体匹配，称为**特征度量一致性**。有趣的是，我们发现，尽管用光度损失训练的立体匹配网络不是最优的，但它的特征提取器可以产生退化不可知（即对IL和IR↑之间退化的鲁棒性）以及对应的非对称像素匹配特定特征（即，图1(b)中的$F_{L}[p_{L}]=F_{R}[p_{R}]$)。然后可以利用这些特征来制定一个特征度量损失，以避免光度不一致。

  + 自增强优化策略：此外，通过特征度量损失对立体匹配网络进行优化，我们可以优化特征提取器，从立体匹配对中捕获更多一致的属性，增强特征度量的一致性。为此，我们引入了一种自增强策略来逐步优化特征提取器。具体地说，**我们使用从前一阶段学习到的特征提取器来形成当前阶段的新的特征度量损失**。这样，我们的方法即使对大的退化仍然有效。

+ 有效性验证：为了定量评估该方法的性能，我们**模拟了四个分辨率不对称的立体数据集**，其中两个来自广泛使用的立体数据集Mederal-Bury和KITTI2015，两个来自Inria_SLFD和HCI的光场数据集，两个视图之间的基线更接近智能手机上的配置。LR视图是在其原始HR版本的不同退化下生成的。为了在真实场景中评估我们的方法，我们用华为P30智能手机远距离摄像机系统收集了一个分辨率不对称的立体数据集。

+ 贡献总结：

  + 第一个从分辨率非对称立体对中估计对应点的**无监督学习方法**；

  + 有效地实现**特征度量一致性**，以避免因未知退化而导致的光度不一致性；

  + 一种通过渐进损失更新来增强特征度量一致性的**自增强策略**；

  + 在**模拟数据集和真实数据集**上，与比较方法相比有明显的性能改进。

<div align="center">
  <img src=".\images\1655793106912.jpg" width="50%" height="50%" />
</div>

## 2. 相关工作

+ 立体匹配：立体匹配，默认为对称的，作为一个经典的计算机视觉任务已经被广泛研究了几十年。近年来，基于深度学习的立体匹配方法已经明显地超越了传统的匹配算法。根据是否需要真值视差图作为标签，这些方法可分为有监督和无监督类别。在许多不容易获得标签的现实世界系统中，无监督的方法使学习能够在没有基本真值信息的情况下进行，其中大多数利用光度一致性的假设来制定光度损失。然而，当立体图像变得不对称时，这种假设就会被违背。

+ 非对称立体匹配。文献中考虑了几种立体匹配的非对称情况，包括**辐射变化**、**模态差异**和**视觉质量不平衡**。为了估计具有辐射变化的立体图像的对应点，提出了不同的鲁棒匹配代价，如互信息度量和自适应归一化互相关。对于跨模态立体，来自两个不同模态的图像被归一化为单个模态，以弥补光度不一致性，例如，通过深度变换网络。近年来，考虑到视觉不平衡（单目模糊和噪声）的立体匹配问题，采用视图合成网络和立体重建网络相结合的方法来解决，这需要真值视差、高质量的去梯度视图版本和显式的超视退化形式。**分辨率不对称可以看作是一种视觉不平衡**，但这种有监督的解决方案很难适用于各种现实系统。

+ 特征度量学习。对于几何任务，有几个开创性的工作利用深度特征作为无监督学习的度量。

  + Zhang等人，将光度损失和基于预先训练特征的特征度量损失进行结合，证明了单目深度估计的性能；

  + Shu等人，利用一个自动编码器和两个正则化损失来学习定制特征；

  + Spencer等人，用对比损失学习特征；

  + Liu等人，提出在一个域变换网络的特征空间中惩罚立体网络的匹配错误

  + 受上述工作的启发，我们**首次将特征度量一致性的概念引入**到分辨率非对称立体匹配的新任务中。

## 3. 准备工作

一对由HR视图和LR视图组成分辨率不对称的立体图像。令左视图$I_{L} \in \mathbb{R}^{H \times W}$为HR，右视图$I_{R} \in \mathbb{R}^{\frac{H}{s} \times \frac{W}{s}}$为LR（s为非对称系数）。使用传统的插值算法（如bicubic）将分辨率对齐，计上采样后的$I_{r}$为$I_{r \uparrow} \in \mathbb{R}^{H \times W}$，尽管经过上采样，但由于$I_{r \uparrow}$中的高频信息不存在，因此$I_{L}$和$I_{r \uparrow}$的立体对仍然是不对称的。

### 3.1 用光度一致性学习

+ 将立体对$I_{L}$和$I_{r \uparrow}$作为输入，无监督立体匹配网络$\Phi(\cdot ; \theta)$预测相对于左视图$I_{L}$的视差图$d_{L}=\Phi(I_{L},I_{r \uparrow} ; \theta)$，基于对应点的光度一致性进行训练，即：

  $$I_{L}[p_{L}]=I_{r \uparrow}[p_{r \uparrow}]                          \tag {1}$$

+ 如果视差$d_{L}[p_{L}]$得到精确地估计，那么左图的$I_{L}[p_{L}]$可以由右图的$I_{r \uparrow}[p_{L}]$结合视差变换得到，即：

  $$I_{r \uparrow  \rightarrow L}[p_{L}]=I_{r \uparrow}[p_{L}-d_{L}[p_{L}]] \tag {2}$$

+ 因此，光度损失可由$I_{L}$及其重建结果$I_{r \uparrow  \rightarrow L}$之间的误差构成，一般是带权重$\alpha$的$L_{1}$和$SSIM$距离的组合，即：

  $$\mathcal{L}_{pm}=\| I_{L}-I_{r \uparrow \rightarrow L} \|_{1}+\alpha (1-SSIM(I_{L},I_{r \uparrow \rightarrow L})) \tag {3}$$

> [SSIM](http://t.csdn.cn/0grJ7)：结构相似性指标（Structural Similarity Index Measure）

### 3.2 挑战与动力

从直观上看，分辨率不对称对无监督立体匹配有双重挑战：

1. 网络特征提取器从非对称输入中提取对称的特征可能更加困难；

2. 光度一致性损失对于非对称输入可能会失去作用。

我们进行了一系列实验来验证这两个因素的影响。在实验中，假设正确视图的真值HR版本的$I_{R}$是可用的，这样就可以控制输入到特征提取器图像的对称性和非对称性以消融因子$1.$和控制用于计算消融因子$2.$的光度损失的图像的对称性或不对称性。

<div align="center">
  <img src=".\images\1655793224607.jpg" width="50%" height="50%" />
</div>


如图2所示，总共评估了四种无监督立体匹配设置，其中只有第一种设置(S1)在实践中可以实现，其余的设置(S2、S3、S4)由于使用了HR右视图，可以认为是“理想情况”。我们从Inria SLFD数据集中选择每个场景的两个视图作为HR左视图和右视图，即$I_{L}$和$I_{R}$。在四个不对称因子(s=2,4,6,8）下，用双三次下采样$I_{R}$模拟了LR右视$I_{r}$。骨干网络采用当下流行的PSMNet作为骨干网络，光度损失采用公式（3），$\alpha=3$，使用标准立体匹配度量——3像素误差（3PE）进行不同设定的性能评估（细节设置详见第5节）。

<div align="center">
  <img src=".\images\1655794348864.jpg" width="50%" height="50%" />
</div>


从表1中可以看出，当输入到特征提取器的图像从非对称变为对称(S1到S2)时，性能的改善相当有限（例如，当s=4时，性能的改善为2.55%）。相反，当用于计算光度损失的图像由不对称变为对称(S1到S3)时，结果有较大的改善（例如，当s=4时，改善幅度为6.24%)，甚至接近上界(S4)。值得强调的是，对于S1和S3，用于右视图变换的视差图来自相同的输入和相同的网络。这种现象在所有非对称因素下都可以观察到。**结果表明，对于分辨率非对称的立体匹配，损失计算过程中的非对称性比输入的非对称性更重要。**

一种弥补光度不对称的可能方法是通过SR技术恢复LR右视图$I_{r \uparrow}$以逼近$I_{R}$。然而，对于不同的现实系统，无论是真实的$(I_{r \uparrow},I_{R})$，还是从$I_{R}$到$I_{r \uparrow}$的显式退化，都不能很容易地用于训练SR模型。因此，该解决方案在适当的模拟数据上可能执行得很好，但在实践中将失去效力。鉴于表1中的结果，我们提出从一个新的角度来克服“非对称损失”的挑战，**通过将$I_{L}$和$I_{r \uparrow}$投影到一个对退化不可知且特定于匹配的特征空间**。一方面，退化不可知空间可以建立另一种一致性（即特征度量一致性）来避免光度不一致性。另一方面，匹配空间可以为不同场景点的像素分配不同的值，因此适合于惩罚不正确的匹配。现在剩下的问题是：如何学习想要的特征空间？

## 4. 分辨率非对称立体匹配

### 4.1 特征空间研究

回顾表1的结果，表明在S3设置下训练的立体匹配网络的有限元提取器能够从非对称输入中提取对称特征。虽然S3在实际应用中并不适用，但它提出了一种潜在的替代方法，即采用相同输入的S1的特征提取器来获得所需的特征空间。为了验证该推测，我们进行了另一系列实验。除了S1之外，我们还研究了另外两个用于几何任务的具有代表性的特征空间：

1. 用对比损失（Contrastive Loss，CL）训练的特征网络；

2. 用自动编码器（Auto-Encoder，AE）网络的编码器。

这两个网络的细节在补充部分中给出。所有这些网络都是在s=4的Inria_SLFD数据集上预先训练的。此外，我们还在原始图像空间中进行了比较。

我们通过计算由相应的网络从$I_{R}$图像中提取的特征图与其退化版本$I_{r \uparrow}$图像之间的PSNR度量来评估不同空间的退化不可知特性。图像空间的PSNR是根据像素强度计算的。注意，不同空间中的值被归一化为[0,1]，以使PSNR结果的比较有意义。另一方面，针对匹配特性，我们直接在不同空间对$I_{L}$和$I_{r \uparrow}$提取的特征映射进行匹配。具体来说，在一定的差距下，我们通过计算两个特征向量的欧几里得距离来构造匹配代价。然后，采用赢者通吃的策略，在每个位置选择匹配代价最小的视差图。对于图像空间，我们在$5 \times 5$贴片的基础上执行匹配。然后使用3PE度量来评估所获得的视差图。

> [PSNR](http://t.csdn.cn/6znJB)：峰值信噪比（Peak Signal-to-Noise Ratio）

<div align="center">
  <img src=".\images\1655797288140.jpg" width="50%" height="50%" />
</div>

表2给出了不同空间的PSNR和3PE结果。虽然CL呈现最高的PSNR值，但在3PE方面表现最差。换句话说，CL的退化不可知性最强，但匹配特异性最弱，这可以归因于特征网络提取的模糊特征映射。由于正则化损失，AE可以学习相对有鉴别性的特征进行匹配，但它不会强加$I_{R}$和$I_{r \uparrow}$特征图之间的一致性，导致PSNR值最低。与光度一致性的图像空间相比S1的特征空间可以为$I_{R}$、$I_{r \uparrow}$分配更一致的特征（PSNR值明显较高）。同时，该特征空间在进行$I_{L}$与$I_{r \uparrow}$匹配时比其它特征空间具有更强的识别能力（3PE匹配效果最好）。**总之我们验证了立体匹配的特征提取器网络可以逼近期望的特征空间，即使训练时存在“非对称损失”**。本文的补充部分对此部分进行了更多的分析，并提出了不同特征图的可视化方法。



### 4.2 特征度量一致性学习

<div align="center">
  <img src=".\images\1655798099257.jpg" width="100%" height="50%" />
</div>

图3说明了我们提出的分辨率非对称立体匹配的方法，它遵循3.1节中描述的典型的无监督学习流程。注意，本工作的重点不是设计一个特定的立体匹配网络，而是实现特征度量的一致性，以避免光度不一致。因此，我们采用流行的PSMNet作为立体匹配网络的主干，它可以很容易地被其他实施例所取代（见第5.3节iResNet的实施例）

图3(a)所示，立体匹配网络$\Phi (\cdot ;\theta _{F} ; \theta _{M})$由特征提取器$\Phi_{F} (\cdot ;\theta _{F} )$和匹配模块$\Phi_{M} (\cdot ; \theta _{M})$构成。给定一个立体对$I_{L}$和$I_{r \uparrow}$，$\Phi (\cdot ;\theta _{F} )$提取退化不可知和匹配特异的特征$F_{L}=\Phi_{F} (I_{L};\theta _{F})$和$F_{r \uparrow}=\Phi_{F} (I_{L};\theta _{F})$，这两个特征在非对称像素的对应点上是一致的，即：

$$F_{L}[p_{L}]=F_{r \uparrow}[p_{r \uparrow}]                            \tag {4}$$

然后，将特征$F_{L}$和$F_{r \uparrow}$串联成一个代价体，并用$\Phi_{M} (\cdot ; \theta _{M})$进行正则化，回归出视差图$d_{L}$。

根据4.1节的调查，提出了使用立体匹配网络的特征提取器其本身产生所需的特征空间，用于计算特征度量损失。具体来说，在获得根据$d_{L}$变换后的左视图$I_{r \uparrow \rightarrow L}$ ,利用特征提取器$\Phi_{F} (\cdot ;\theta _{F} )$将$I_{L}$和$I_{r \uparrow \rightarrow L}$ 投影到特征空间,得到$F_{L}$和$F_{r \uparrow \rightarrow  L}=\Phi_{F} (I_{r \uparrow \rightarrow L} ;\theta _{F} )$。

由于$F_{L}$应当由$F_{r \uparrow \rightarrow  L}$和$d_{L}$很好地重建，可以仿照光度一致性损失构建特征度量损失$\mathcal{L}_{fm}$：

$$\mathcal{L}_{fm}=\| F_{L}-F_{r \uparrow \rightarrow L} \|_{1}+\alpha (1-SSIM(F_{L},F_{r \uparrow \rightarrow L})) \tag {5}$$

### 4.3 自增强策略

正如4.1节所展示的那样，即使用光度损失$\mathcal{L}_{pm}$训练立体匹配网络$\Phi (\cdot ;\theta _{F} ; \theta _{M})$，其特征提取器$\Phi_{F} (\cdot ;\theta _{F} )$也能逼近期望的特征空间。然而，当网络以更精确的损失（如$\mathcal{L}_{fm}$）训练时，对应点的$\Phi_{F} (\cdot ;\theta _{F} )$提取出更多退化不可知和匹配特定的特征，这些特征可以用来加强特征度量的一致性，并形成更好的$\mathcal{L}_{fm}$。反过来，一个更好的$\mathcal{L}_{fm}$可以进一步提高$\Phi (\cdot ;\theta _{F} ; \theta _{M})$。为此，我们提出了一种自增强机制来逐步优化特征提取器，并对网络进行增强。

图3(b)说明了我们方法的训练过程。给定一个分辨率非对称的立体数据集，我们首先使用 $\mathcal{L}_{pm}$训练一个立体匹配网络$\Phi (\cdot ;\theta^{0} _{F} ; \theta^{0} _{M})$（简称$\Phi^{0}$)，其特征提取器$\Phi^{0}_{F}$形成特征度量损失$\mathcal{L}^{0}_{fm}$。然后，利用$\mathcal{L}^{0}_{fm}$优化了一个新的立体匹配网络$\Phi^{1}$。在$\Phi^{1}$调整的过程中，用于计算$\mathcal{L}^{0}_{fm}$的特征提取器是固定的。调整完成后，增强后的$\Phi^{1}_{F}$又可以形成更好的特征度量损失$\mathcal{L}^{1}_{fm}$（将用于下一步增强）。不断利用$\mathcal{L}^{k-1}_{fm}$来调整$\Phi^{k}$，其中$k \in 1,...,K$。注意，我们只在网络$\Phi^{k}$收敛于$\mathcal{L}^{k-1}_{fm}$时才构建新的损失$\mathcal{L}^{k}_{fm}$，因为频繁改变损失空间可能会使训练过程不稳定。通过这种自增强策略，我们可以获得具有逐步增强的特征度量一致性的连续优化网络。详细算法见支持材料。

<div align="center">
  <img src = ".\images\1655803241399.png" width="50%" height="50%" />
</div>

为了验证所提出的策略，我们在s=4的Inria_SLFD数据集上评估了立体匹配网络在不同阶段的性能。如表3所示，网络随着阶段的增加而逐步改善。这反映了下一阶段使用的特征提取器得到了增强，特征度量的一致性得到了加强。此外，有了这样一种策略，我们的方法对于大规模降解仍然有效。我们用两个更大的不对称因素（s=6，8）来验证这一说法。如表3所示，当不对称因子增加时，由于光度不一致性更严重，初始网络（k=0）的性能显著恶化。然而，由于自增强策略，网络最终达到了不错的性能。

## 5. 在模拟数据集上的实验

### 5.1 数据集和评估指标

+ 数据集：为了定量评估我们方法的性能，我们模拟了四个分辨率不对称的立体数据集，两个来自广泛使用的立体数据集Middlebury和KITTI2015，两个来自Inria_SLFD和HCI的光场数据集，两个视图之间的基线较窄，更接近于智能手机的基线设置。为了模拟真实世界系统中多样的退化模式，我们执行五种不同的退化操作来合成LR视图，包括双三次下采样（BIC）、各向同性/各向异性高斯核下采样（IG/AG）和各向同性/各向异性高斯核JPEG压缩下采样（IG JPEG/AG JPEG）。支持材料中提供了每个数据集的训练/测试划分以及不同高斯核的生成的详细信息。

+ 评估指标：3像素误差（3PE）和终点误差（EPE）。

  + 3PE是全部区域误差超过3像素且超过真实值5%大小的异常点百分比；

  + EPE是估计视差和真实视差之间的平均绝对差异。

### 5.2 比较方法

为了进行比较，我们采用了经典的立体匹配方法半全局匹配（SGM）和几种可分为两类的无监督方法。

+ 第一类包括三种使用光度损失的解决方案。除了在S1设置下训练的基准无监督网络（表示为BaseNet）外，我们进一步使用最先进的非盲SR方法RCAN和盲SR方法DAN超分辨率化LR视图作为预处理，分别表示为RCAN+BaseNet和DAN+BaseNet。RCAN模型在大规模数据集DIV2K上的BIC退化下训练，而DAN模型在DIV2K上的一组退化下训练，包括BIC、IG和AG。

+ 第二类包括两种特征度量学习方法，它们也采用基线网络，但在各自的特征空间中施加特征度量一致性，分别表示为BaseNet+CL和BaseNet+AE。

> 请注意，除非使用SR模型，否则将应用双三次插值对LR视图进行上采样。

所有基于学习的解决方案的骨干网络都是流行的PSMNet。使用ADAM解算器优化网络（$\beta 1=0.9,\beta 2=0.999$）我们将学习率设置为0.001。视差的平滑约束是通过加权平滑度损失来实现的，即：

$$\mathcal{L}_{sm}=|\partial_{x}d_{L}|e^{-|\partial_{x}I_{L}|}+|\partial_{y}d_{L}|e^{-|\partial_{y}I_{L}|}   \tag {6}$$

因此，所有基于学习的解决方案的总体损失函数可以写成：

$$\mathcal{L}=\mathcal{L}_{pm/fm}+\lambda \mathcal{L}_{sm}                   \tag {7}$$

式中，$\lambda$是权重因子，$\mathcal{L}_{pm/fm}$是第一类方法的光度损失，或是第二类方法和我们的方法的相应特征度量损失。自增强策略中的阶段K数设置为3。补充材料中提供了骨干网的详细架构和不同方法的超参数。

### 5.3 结果

+ 定量结果：

  <div align="center">
    <img src = ".\images\1655864025900.jpg" width="100%" height="50%" />
  </div>

  + 表4显示了不对称因子为4的四个模拟数据集上不同方法的比较结果。与不假设特定退化的方法（SGM、BaseNet、BaseNet+CL和BaseNet+AE）相比，我们的方法在所有数据集和所有退化情况下都具有明显的优势。虽然BaseNet+CL/AE也采用了特征度量损失，但其性能仅与BaseNet相当，甚至不如BaseNet。它告诉我们，找到一个退化感知和匹配的特定特征空间是非常重要的。与退化特定SR解决方案RCAN+BaseNet和DAN+BaseNet的比较应从两个方面进行解释。一方面，当实际退化与假设一致时（RCAN的BIC和BIC/IG/AG for DAN）我们的方法在大多数情况下都有更好的性能，但提升并没有那么大；另一方面，当实际退化与其假设不一致时（表4中标记为灰色），我们的方法明显优于这些SR解。**也就是说，当真实场景中的降解未知时，SR解决方案将失去效力。**

+ 定性结果：

  <div align="center">
  <img src = ".\images\1655864598466.jpg" width="100%" height="50%" />
  </div>

  + 我们在图4中提供了HCI和Middlebury数据集中的两个示例场景的视觉结果，以供比较。可以看出，我们的方法获得了更稳健的结果，尤其是在深度不连续的区域。在这些地区，基于光度一致性的解决方案的对应估计具有挑战性，因为即使借助SR技术，匹配模糊度也无法解决。相比之下，在退化不可知和匹配特定特征空间中施加的特征度量一致性下，我们的方法比BaseNet+CL/AE更好地揭示了测试场景的三维几何结构。

+ 大的不对称因素：

  <div align="center">
    <img src = ".\images\1655864770606.jpg" width="100%" height="50%" />
  </div>

  + 为了评估不同方法在大退化情况下的性能，我们在不对称因子为8的模拟数据集上以及在BIC退化情况下进行了实验。从表5可以看出，我们的方法大大优于所有比较方法，与表4的结果相比，提升幅度更大。对于使用光度损失的方法，由于更严重的光度不一致性，其性能进一步恶化。相比之下，由于采用了自增强策略，我们的方法逐渐增强了特征度量的一致性，从而保持了优异的性能。

+ 与监督学习的比较：

  <div align="center">
    <img src = ".\images\1655865033818.jpg" width="100%" height="50%" />
  </div>

  + 这项工作的重点是无监督学习，它在训练期间不需要真值视差标签，并且在不同的真实世界系统中部署时更加稳健。为了验证这一点，我们还实现了一种有监督的方法，该方法使用与我们相同的主干网络，但利用真值视差来计算平滑的L1损失（表示为BaseNet-su）。我们在Middlebury和KITTI2015数据集上进行了实验，在IG退化条件下，非对称因子为4。对于这两个数据集，网络都在Middlebury里进行训练。由于KITTI2015由街道场景组成，而Middlebury由室内场景组成，这两个数据集有很大的领域差距。如表6所示，当在Middlebury上训练并在同一数据集上测试时，BaseNet-su的性能最好，这是合理的。然而，当在Middlebury进行培训并在KITTI2015上进行测试时，有监督的方法失去了效力，我们的方法实现了明显更好的泛化，证明了我们的方法在实际场景中的稳健性，在实际场景中，视差标签不可用于训练。

+ 骨干网研究：

  <div align="center">
    <img src = ".\images\1655865435478.jpg" width="100%" height="50%" />
  </div>

  + 除了PSMNet由于采用了3D卷积层外，我们还研究了iResNet作为我们方法的主干网络的另一个实施例，它完全基于2D卷积层。实验在不对称因子为4的BIC降解条件下进行。如表7所示，我们的方法的iResNet版本显示，在所有数据集上，与使用光度损失训练的基线网络相比，有显著的提升。它表明，iResNet的特征提取器还学习退化不可知和匹配的特定特征，可用于建立特征度量的一致性。换句话说，我们方法的有效性与所使用的骨干网络无关。

## 6. 真实数据集上的实验

+ 数据集准备：
  
+ 为了验证我们的方法在真实系统中的性能，我们收集了一个具有真实退化的分辨率非对称立体数据集。不对称立体对由华为P30智能手机捕捉。这部智能手机配备了一个远程宽摄像头系统，包括一个27mm的等效主镜头和一个80mm的等效远程照相镜头。不对称因子约等于3。经过摄像机标定和立体校正，我们为室内外场景捕获了30对不对称立体对。我们随机分成5对作为测试集，其他作为训练集。
  
+ 结果：

  <div align="center">
    <img src = ".\images\image-20220622104231447.png" width="100%" height="50%" />
  </div>
  
  + 如图5所示，与竞争对手相比，我们的方法获得了最佳的视觉质量。与模拟数据集上的结果类似，我们的方法估计出更尖锐的边缘，并更好地分离属于不同深度级别的对象。这种优势对于下游应用至关重要，例如**bokeh**和**3D摄影**。相反，使用光度损失的方法将一些不需要的纹理从输入图像复制到估计的视差贴图（例如雨伞表面），这主要是由于立体匹配过程中光度不一致造成的。两种SR解决方案在基准网络上的改善微不足道，因为它们的退化假设与实际情况不同。另外，另外两种使用特征度量损失的方法由于其特征空间不足而产生了不令人满意的结果。补充部分提供了更多结果。

## 7. 限制和结论

+ 限制：

  + 由于两个镜头固有的光学差异，在使用远程广角相机系统拍摄立体图像时，除了分辨率之外，还可能存在其他类型的不对称性（例如颜色和亮度）。在收集真实数据集时，我们手动调整两个镜头的ISO、曝光时间和白平衡，以缓解这些问题。虽然在全局配准后，通过显式的颜色和亮度校正可以进一步缓解这些问题，但是否可以通过扩展所提出的方法直接解决其他类型的不对称问题仍然是一个悬而未决的问题。我们将把它视为今后的工作。

+ 结论：

  + 在本文中，我们揭示了从分辨率对称立体图像进行无监督对应估计的主要挑战，即违反光度一致性。为了克服这一挑战，我们以一种有效的方式实现了特征度量的一致性，并引入了一种自增强策略来增强这种一致性。通过综合实验验证，我们的方法在处理实际中两个视图之间的各种退化方面表现出了优异的性能。

